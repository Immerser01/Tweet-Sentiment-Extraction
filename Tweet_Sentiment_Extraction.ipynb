{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09U0DNS1KTp-",
        "outputId": "8f4eaadf-9496-4d5e-bfa1-f0bb3b1dbb47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9QS62wuKbOw",
        "outputId": "20735902-b28d-450b-f4b5-5d4f05fc55dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'My Drive/Practice'\n",
            "/content/drive/My Drive/Practice\n"
          ]
        }
      ],
      "source": [
        "cd \"My Drive/Practice\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my2O8Hz9Kfzv",
        "outputId": "bec95acc-3543-4fc3-f5ca-c495553e0af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi1uddIoKiC8"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import statistics\n",
        "import tokenizers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
        "from transformers import BertPreTrainedModel, RobertaConfig, RobertaModel, AdamW\n",
        "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMfi8dzcKlx6"
      },
      "outputs": [],
      "source": [
        "max_len = 108\n",
        "hidden_size = 768\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "lr = 2.5e-5\n",
        "dropout_rate = 0.0\n",
        "hidden_dropout_prob = 0.1\n",
        "attention_probs_dropout_prob = 0.2\n",
        "num_classes = 2\n",
        "n_splits = 5\n",
        "random_seed = 42\n",
        "warmup_steps = 199\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab='vocab.json', \n",
        "    merges='merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms5RopCxLDq0"
      },
      "outputs": [],
      "source": [
        "chars = [\".\", \"!\", \"?\"]\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def find_sub_list(l,sl):\n",
        "    \"\"\"\n",
        "    returns all occurences of sublist sl in list l\n",
        "    i.e. their start and end position\n",
        "    e.g.[(0, 4), (5, 9)]\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(sl) == 0:\n",
        "        return []\n",
        "    \n",
        "    else:\n",
        "        results=[]\n",
        "        sll=len(sl)\n",
        "        for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "            if l[ind:ind+sll]==sl:\n",
        "                results.append((ind,ind+sll))\n",
        "\n",
        "        return results\n",
        "    \n",
        "def findOccurrences(s, ch):\n",
        "    occ = [i for i, letter in enumerate(s) if letter == ch]\n",
        "    return occ, len(occ)\n",
        "\n",
        "def preprocess_train(train_df):\n",
        "\n",
        "    # make a copy of original text and selected_text\n",
        "    train_df[\"preprocessed_text\"] = train_df[\"text\"]\n",
        "    train_df[\"preprocessed_selected_text\"] = train_df[\"selected_text\"]\n",
        "\n",
        "    # add space in the beggining\n",
        "    train_df[\"preprocessed_text\"] = train_df[\"preprocessed_text\"].apply(lambda x: \" \" + x if x[0] != \" \" else x)\n",
        "    \n",
        "\n",
        "    # Sample: bye... -> bye. . .\n",
        "    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n",
        "                                              train_df.preprocessed_selected_text.values)):\n",
        "        spaces = []\n",
        "    \n",
        "        for char in chars:\n",
        "            idxs, num_idxs = findOccurrences(text, char)\n",
        "            if num_idxs > 1:\n",
        "                for pos in range(num_idxs-2, -1, -1):\n",
        "                    if idxs[pos] == idxs[pos+1]-1:\n",
        "                        text = text[:idxs[pos]+1] + \" \" + text[idxs[pos]+1:]\n",
        "                        spaces.append(char)\n",
        "        if any(spaces):\n",
        "            train_df.loc[i, \"preprocessed_text\"] = text\n",
        "    \n",
        "        for char in chars:\n",
        "            idxs, num_idxs = findOccurrences(selected_text, char)\n",
        "            if num_idxs > 1:\n",
        "                for pos in range(num_idxs-2, -1, -1):\n",
        "                    if idxs[pos] == idxs[pos+1]-1:\n",
        "                        selected_text = selected_text[:idxs[pos]+1] + \" \" + selected_text[idxs[pos]+1:]\n",
        "    \n",
        "        train_df.loc[i, \"preprocessed_selected_text\"] = selected_text\n",
        "    \n",
        "    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n",
        "                                              train_df.preprocessed_selected_text.values)):\n",
        "       \n",
        "        text_tokenized = tokenizer.encode(text).tokens\n",
        "        selected_tokenized = tokenizer.encode(selected_text).tokens\n",
        "        subi_spl = find_sub_list(text_tokenized, selected_tokenized)\n",
        "    \n",
        "        selected_tokenized1 = tokenizer.encode(\" \" + selected_text).tokens\n",
        "        subi_spl1 = find_sub_list(text_tokenized, selected_tokenized1)\n",
        "        \n",
        "        if len(subi_spl) == 0 and len(subi_spl1) > 0:\n",
        "            train_df.loc[i, \"preprocessed_selected_text\"] = \" \" + selected_text\n",
        "        \n",
        "\n",
        "    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n",
        "                                              train_df.preprocessed_selected_text.values)):\n",
        "       \n",
        "        text_tokenized = tokenizer.encode(text).tokens\n",
        "        selected_tokenized = tokenizer.encode(selected_text).tokens\n",
        "        subi_spl = find_sub_list(text_tokenized, selected_tokenized)\n",
        "    \n",
        "        if len(subi_spl) == 0:\n",
        "            start = text.find(selected_text)\n",
        "            end = start + len(selected_text)\n",
        "            for s in range(start-1, -1, -1):\n",
        "                if text[s] != \" \":\n",
        "                    selected_text = text[s] + selected_text\n",
        "                else:\n",
        "                    selected_text = \" \" + selected_text\n",
        "                    break\n",
        "            for s in range(end, len(text)):\n",
        "                if text[s] != \" \":\n",
        "                    selected_text = selected_text + text[s] \n",
        "                else:\n",
        "                    break\n",
        "        train_df.loc[i, \"preprocessed_selected_text\"] = selected_text\n",
        "        \n",
        "    \n",
        "    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n",
        "                                              train_df.preprocessed_selected_text.values)):\n",
        "       \n",
        "        text_tokenized = tokenizer.encode(text).tokens\n",
        "        selected_tokenized = tokenizer.encode(selected_text).tokens\n",
        "        subi_spl = find_sub_list(text_tokenized, selected_tokenized)\n",
        "        \n",
        "        if len(subi_spl) == 0:\n",
        "            train_df.drop(i, inplace=True)\n",
        "\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def preprocess_test(test_df):\n",
        "    # I STEP\n",
        "    # make a copy of original text and selected_text\n",
        "    test_df[\"preprocessed_text\"] = test_df[\"text\"]\n",
        "    # add space at the begginings\n",
        "    test_df[\"preprocessed_text\"] = test_df[\"preprocessed_text\"].apply(lambda x: \" \" + x if x[0] != \" \" else x)   \n",
        "    \n",
        "    # II STEP\n",
        "    for i, text in enumerate(test_df.preprocessed_text.values):\n",
        "        spaces = []\n",
        "        for char in chars:\n",
        "            idxs, num_idxs = findOccurrences(text, char)\n",
        "            if num_idxs > 1:\n",
        "                for pos in range(num_idxs-2, -1, -1):\n",
        "                    if idxs[pos] == idxs[pos+1]-1:\n",
        "                        text = text[:idxs[pos]+1] + \" \" + text[idxs[pos]+1:]\n",
        "                        spaces.append(char)\n",
        "        if any(spaces):\n",
        "            test_df.loc[i, \"preprocessed_text\"] = text    \n",
        "    \n",
        "def get_labels(tokens, pr_selected_text):\n",
        "    \n",
        "    start_labels = [0] * len(tokens)\n",
        "    end_labels = [0] * len(tokens)\n",
        "\n",
        "    selected_tokens = tokenizer.encode(pr_selected_text).tokens\n",
        "    subi = find_sub_list(tokens, selected_tokens)\n",
        "    assert len(subi) > 0, \"Something is wrong!!!\"\n",
        "    \n",
        "    start, end = subi[0]\n",
        "    start_labels[start] = 1\n",
        "    end_labels[end-1] = 1\n",
        "    \n",
        "    start_labels = [0] * 4 + start_labels + [0]\n",
        "    end_labels = [0] * 4 + end_labels + [0]\n",
        "\n",
        "    return start_labels, end_labels\n",
        "\n",
        "\n",
        "def process_sample(text, pr_text, sentiment, max_len, tokenizer, selected_text=None, pr_selected_text=None):\n",
        "    \"\"\"\n",
        "    gets all info for one sample\n",
        "    \"\"\"\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    info = tokenizer.encode(pr_text)\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + info.ids + [2]    \n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    tokens = info.tokens\n",
        "    offsets = info.offsets\n",
        "    \n",
        "    loss_mask = [0] * 4 + [1] * len(tokens) + [0]\n",
        "                \n",
        "    if selected_text is not None:\n",
        "        start_labels, end_labels = get_labels(tokens, pr_selected_text)\n",
        "    else: start_labels, end_labels = None, None\n",
        "        \n",
        "    pad_len = max_len - len(input_ids)\n",
        "    if pad_len > 0:\n",
        "        input_ids = input_ids + ([1] * pad_len)\n",
        "        attention_mask = attention_mask + ([0] * pad_len)\n",
        "        loss_mask = loss_mask + ([0] * pad_len)\n",
        "        \n",
        "        if selected_text is not None:\n",
        "            start_labels = start_labels + ([0] * pad_len)\n",
        "            end_labels = end_labels + ([0] * pad_len)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"loss_mask\": loss_mask,\n",
        "        \"tokens\": tokens,\n",
        "        \"start_labels\": start_labels,\n",
        "        \"end_labels\": end_labels,\n",
        "        \"offsets\": offsets,\n",
        "        \"text\": text,\n",
        "        \"preprocessed_text\": pr_text,\n",
        "        \"selected_text\": selected_text,\n",
        "        \"preprocessed_selected_text\": pr_selected_text,\n",
        "        \"sentiment\": sentiment\n",
        "    }\n",
        "\n",
        "def tweet_loss(start_logits, start_labels, end_logits, end_labels, loss_mask):\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # mask actives\n",
        "    actives = loss_mask.reshape(-1) == 1\n",
        "    # apply the masks\n",
        "    start_logits = start_logits.reshape(-1)[actives]\n",
        "    start_labels = start_labels.reshape(-1)[actives]\n",
        "    end_logits = end_logits.reshape(-1)[actives]\n",
        "    end_labels = end_labels.reshape(-1)[actives]\n",
        "        \n",
        "    start_loss =  loss_fn(start_logits, start_labels.type_as(start_logits))\n",
        "    end_loss =  loss_fn(end_logits, end_labels.type_as(end_logits))    \n",
        "    \n",
        "    return (start_loss + end_loss) / 2\n",
        "\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "class TweetDataset:\n",
        "    \n",
        "    max_len = max_len\n",
        "    tokenizer = tokenizer\n",
        "    \n",
        "    def __init__(self, df, sentiment_values=None):\n",
        "        if sentiment_values is not None:\n",
        "            df = df.loc[df.sentiment.isin(sentiment_values)].reset_index(drop=True)\n",
        "        self.df = df\n",
        "        self.indexs = df.index.values\n",
        "        self.text = df.text.values\n",
        "        self.preprocessed_text = df.preprocessed_text.values\n",
        "        self.sentiments = df.sentiment.values\n",
        "        if \"selected_text\" in df:\n",
        "            self.selected_text = df.selected_text.values\n",
        "            self.preprocessed_selected_text = df.preprocessed_selected_text.values\n",
        "        else:\n",
        "            self.selected_text = None\n",
        "            self.preprocessed_selected_text = None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.indexs)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        if self.selected_text is not None:\n",
        "            return process_sample(self.text[idx], self.preprocessed_text[idx], self.sentiments[idx], self.max_len,\n",
        "                                self.tokenizer, self.selected_text[idx], self.preprocessed_selected_text[idx])\n",
        "        else:\n",
        "            return process_sample(self.text[idx], self.preprocessed_text[idx], self.sentiments[idx], self.max_len,\n",
        "                                self.tokenizer)\n",
        "    \n",
        "    def get_metadata(self):\n",
        "        \n",
        "        return [self[i] for i in range(len(self))]\n",
        "    \n",
        "    def get_train_dataloader(self, idxs):\n",
        "        \n",
        "        input_ids_all = []\n",
        "        attention_mask_all = []\n",
        "        loss_mask_all = []\n",
        "        start_labels_all = []\n",
        "        end_labels_all = []\n",
        "        \n",
        "        for i in idxs:\n",
        "            \n",
        "            sample = self[i]\n",
        "        \n",
        "            input_ids_all.append(sample[\"input_ids\"])\n",
        "            attention_mask_all.append(sample[\"attention_mask\"])\n",
        "            loss_mask_all.append(sample[\"loss_mask\"])\n",
        "            start_labels_all.append(sample[\"start_labels\"])\n",
        "            end_labels_all.append(sample[\"end_labels\"])\n",
        "\n",
        "        data = TensorDataset(torch.LongTensor(input_ids_all),\n",
        "                             torch.FloatTensor(attention_mask_all),\n",
        "                             torch.LongTensor(loss_mask_all),\n",
        "                             torch.LongTensor(start_labels_all),\n",
        "                             torch.LongTensor(end_labels_all)\n",
        "                                    )\n",
        "        \n",
        "        sampler = SequentialSampler(data)\n",
        "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size) \n",
        "\n",
        "        return dataloader\n",
        "    \n",
        "    def get_test_dataloader(self):\n",
        "        \n",
        "        input_ids_all = []\n",
        "        attention_mask_all = []\n",
        "        loss_mask_all = []\n",
        "        \n",
        "        for i in self.indexs:\n",
        "            \n",
        "            sample = self[i]\n",
        "        \n",
        "            input_ids_all.append(sample[\"input_ids\"])\n",
        "            attention_mask_all.append(sample[\"attention_mask\"])\n",
        "            loss_mask_all.append(sample[\"loss_mask\"])\n",
        "\n",
        "        data = TensorDataset(torch.LongTensor(input_ids_all),\n",
        "                             torch.FloatTensor(attention_mask_all),\n",
        "                             torch.LongTensor(loss_mask_all)\n",
        "                                  )\n",
        "        \n",
        "        sampler = SequentialSampler(data)\n",
        "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size) \n",
        "\n",
        "        return dataloader\n",
        "    \n",
        "class TweetModel(BertPreTrainedModel):\n",
        "    \n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = RobertaModel.from_pretrained('pytorch_model.bin', config=conf)\n",
        "        self.drop_out = nn.Dropout(dropout_rate)\n",
        "        self.clf = nn.Linear(hidden_size, num_classes)\n",
        "        nn.init.normal_(self.clf.weight, std=0.02)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \n",
        "        sequence_outputs, _ = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=False\n",
        "        )\n",
        "                \n",
        "        sequence_outputs = self.drop_out(sequence_outputs)\n",
        "        logits = self.clf(sequence_outputs)\n",
        "        \n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        \n",
        "        return start_logits, end_logits\n",
        "\n",
        "def prepare_predictions(pp):\n",
        "    for char in chars:\n",
        "        idxs, num_idxs = findOccurrences(pp, char)\n",
        "        if num_idxs > 1:\n",
        "            for pos in range(num_idxs-2, -1, -1):\n",
        "                if idxs[pos] == idxs[pos+1] - 2 :\n",
        "                    pp = pp[:idxs[pos]+1] + pp[idxs[pos+1]:]\n",
        "             \n",
        "    return pp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0af-1xyp4gO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_DDjn_uL5lL",
        "outputId": "5cd84379-6acb-4264-de1f-5f293103b2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data: (27480, 4)\n",
            "test data: (3534, 3)\n"
          ]
        }
      ],
      "source": [
        "seed_everything(random_seed)\n",
        "\n",
        "train_path = \"train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "\n",
        "train_raw = pd.read_csv(train_path).dropna().reset_index(drop=True)\n",
        "test_raw = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"train data: {train_raw.shape}\")\n",
        "print(f\"test data: {test_raw.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwnyFCJ-MExL",
        "outputId": "e093745b-7630-485f-c377-e704f2e7a97a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "non neutral train data: (16363, 4)\n",
            "non neutral test data: (2104, 3)\n"
          ]
        }
      ],
      "source": [
        "# get only negative and positive sentiment samples\n",
        "train_df = train_raw.loc[train_raw.sentiment != \"neutral\"].reset_index(drop=True, inplace=False)\n",
        "test_df = test_raw.loc[test_raw.sentiment != \"neutral\"].reset_index(drop=True, inplace=False)\n",
        "print(f\"non neutral train data: {train_df.shape}\")\n",
        "print(f\"non neutral test data: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QJ0hz5oPvEMg",
        "outputId": "65128a0e-7ab0-4acf-8c5c-038caaaf11bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6a2b820-15ca-4c83-aef0-65076ced4c49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6e0c6d75b1</td>\n",
              "      <td>2am feedings for the baby are fun when he is a...</td>\n",
              "      <td>fun</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6a2b820-15ca-4c83-aef0-65076ced4c49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6a2b820-15ca-4c83-aef0-65076ced4c49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6a2b820-15ca-4c83-aef0-65076ced4c49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       textID                                               text  \\\n",
              "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
              "1  088c60f138                          my boss is bullying me...   \n",
              "2  9642c003ef                     what interview! leave me alone   \n",
              "3  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
              "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
              "\n",
              "    selected_text sentiment  \n",
              "0        Sooo SAD  negative  \n",
              "1     bullying me  negative  \n",
              "2  leave me alone  negative  \n",
              "3   Sons of ****,  negative  \n",
              "4             fun  positive  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kt5vN2vnvHEc",
        "outputId": "d098a0c0-9088-4c93-98c5-7ef952d4327a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-80cab41b-9ebf-41bc-b007-c03751b55cf5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>Shanghai is also really exciting (precisely -...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>726e501993</td>\n",
              "      <td>that`s great!! weee!! visitors!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80cab41b-9ebf-41bc-b007-c03751b55cf5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-80cab41b-9ebf-41bc-b007-c03751b55cf5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-80cab41b-9ebf-41bc-b007-c03751b55cf5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       textID                                               text sentiment\n",
              "0  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
              "1  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
              "2  01082688c6                                        happy bday!  positive\n",
              "3  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive\n",
              "4  726e501993                    that`s great!! weee!! visitors!  positive"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "htbvaVWmvL9-",
        "outputId": "036fc565-eeea-404b-df84-aaed2989b3b9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "f, axes = plt.subplots(1, 2,figsize=(15,5))\n",
        "\n",
        "sb.countplot(x='sentiment',data=train_raw,order=train_raw.sentiment.value_counts().index,ax=axes[0]).set_title('Count of various sentiment tweets in training data ')\n",
        "\n",
        "sb.countplot(x='sentiment',data=test_raw,order=test_raw.sentiment.value_counts().index,ax=axes[1])\n",
        "plt.title('Count of various sentiment tweets in test data');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR-gVXTyHu_t",
        "outputId": "f595bb6d-07fb-43e3-ed79-61d628b745fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Lowercase\n",
        "\n",
        "train_df['text']= train_df['text'].apply(lambda x: x.lower())\n",
        "test_df['text']= test_df['text'].apply(lambda x: x.lower())\n",
        "train_df['selected_text']= train_df['selected_text'].apply(lambda x: x.lower())\n",
        "\n",
        "#Removing Hyper-Links\n",
        "import re\n",
        "def remove_hyperlinks(text):\n",
        "  hyperlinkfree=re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "  return hyperlinkfree\n",
        "train_df['text']=train_df['text'].apply(lambda x:remove_hyperlinks(x))\n",
        "test_df['text']=test_df['text'].apply(lambda x:remove_hyperlinks(x))\n",
        "train_df['selected_text']=train_df['selected_text'].apply(lambda x:remove_hyperlinks(x))\n",
        "\n",
        "#Removing Numbers, Angular Brackets, Square Brackets, \\n character\n",
        "#Replacing **** by <ABUSE> word\n",
        "def remove(text):\n",
        "  text=re.sub('\\S*\\d\\S*',' ',text) #Removing Numbers\n",
        "  text=re.sub('<.*?>+',' ',text)   #Removing Angular Brackets\n",
        "  text=re.sub('\\[.*?\\]',' ',text)  #Removing Square Brackets\n",
        "  text=re.sub('\\n',' ',text)       #Removing '\\n' character \n",
        "  text=re.sub('\\*+','<ABUSE>',text) #Replacing **** by ABUSE word\n",
        "  return text\n",
        "\n",
        "train_df['text']=train_df['text'].apply(lambda x:remove(x))\n",
        "test_df['text']=test_df['text'].apply(lambda x:remove(x))\n",
        "train_df['selected_text']=train_df['selected_text'].apply(lambda x:remove(x))\n",
        "\n",
        "#Spelling Correction\n",
        "def wrong_words(text,selected):\n",
        "  words=[]\n",
        "  text=text.split()\n",
        "  selected=selected.split()\n",
        "  for i in selected:dx\n",
        "    if i not in text:\n",
        "      words.append(i)\n",
        "  if len(words)>0:\n",
        "    return \" \".join(words)\n",
        "  else:\n",
        "    return '++++'\n",
        "train_df['spelling']=train_df.apply(lambda x: wrong_words(x.text,x.selected_text),axis=1)\n",
        "\n",
        "def remove_text(x):\n",
        "  selected=x[0]\n",
        "  spelling=x[1]\n",
        "  selected=selected.split()\n",
        "  selected.remove(spelling) \n",
        "  return \" \".join(selected)\n",
        "train_df['selected_text']=train_df[['selected_text','spelling']].apply(lambda x: remove_text(x) if len(x['spelling'])==1  else x['selected_text'],axis=1)\n",
        "train_df['spelling']=train_df.apply(lambda x: wrong_words(x.text,x.selected_text),axis=1)\n",
        "\n",
        "!pip install fuzzywuzzy\n",
        "\n",
        "from fuzzywuzzy import fuzz\n",
        "def matching(x):\n",
        "  text=x[0]\n",
        "  selected=x[1]\n",
        "  spelling=x[2]\n",
        "  text=text.split()\n",
        "  selected=selected.split()\n",
        "  spelling=spelling.split()\n",
        "  for s in spelling:\n",
        "    for t in text:\n",
        "      if s in selected:\n",
        "        if(fuzz.ratio(t,s)>55): \n",
        "          index=selected.index(s)\n",
        "          selected[index]=t\n",
        "  return \" \".join(selected)   \n",
        "train_df['selected_text']=train_df[['text','selected_text','spelling']].apply(lambda x: matching(x) if x['spelling']!='++++'  else x['selected_text'],axis=1)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRDn6cngMHS3",
        "outputId": "dacf90fd-b7dd-412a-fcac-93fd03ed21ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 44.9 s, sys: 1.83 s, total: 46.7 s\n",
            "Wall time: 49 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "preprocess_train(train_df)\n",
        "preprocess_test(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jP7hf9DMJg9"
      },
      "outputs": [],
      "source": [
        "train_dataset = TweetDataset(train_df)\n",
        "test_dataset = TweetDataset(test_df)\n",
        "\n",
        "train_metadata = train_dataset.get_metadata()\n",
        "test_metadata = test_dataset.get_metadata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIDJkpDbML8d"
      },
      "outputs": [],
      "source": [
        "def train_loop():\n",
        "    \n",
        "    # create splits\n",
        "    kf = StratifiedKFold(n_splits=n_splits, random_state=random_seed, shuffle=True)\n",
        "    \n",
        "    # lists to store oof predictions\n",
        "    val_start_logits = [0 for tm in train_metadata]\n",
        "    val_end_logits = [0 for tm in train_metadata]\n",
        "    \n",
        "    # lists to accumulate folds test set predictions\n",
        "    test_start_logits = [torch.zeros(len(tm[\"input_ids\"]), dtype=torch.float) for tm in test_metadata]\n",
        "    test_end_logits = [torch.zeros(len(tm[\"input_ids\"]), dtype=torch.float) for tm in test_metadata]\n",
        "    \n",
        "    test_dataloader = test_dataset.get_test_dataloader()\n",
        "    for fold_idx, (train_idxs, val_idxs) in enumerate(kf.split(X=train_dataset.df, \n",
        "                                                               y=train_dataset.df.sentiment.values)):\n",
        "                        \n",
        "        # create dataloaders\n",
        "        train_dataloader = train_dataset.get_train_dataloader(train_idxs)\n",
        "        val_dataloader = train_dataset.get_train_dataloader(val_idxs)\n",
        "        \n",
        "        # model init\n",
        "        model_config = RobertaConfig.from_pretrained('config.json', lowercase=True)\n",
        "        model_config.hidden_dropout_prob = hidden_dropout_prob\n",
        "        model_config.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        model = TweetModel(conf=model_config)\n",
        "        model.cuda()\n",
        "        \n",
        "        # optimizer init\n",
        "        optimizer = AdamW(model.parameters(), lr=lr)\n",
        "        num_train_steps = epochs * len(train_dataloader)\n",
        "        # create the learning rate scheduler\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, \n",
        "                                            num_train_steps)\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            \n",
        "            model.train()\n",
        "            \n",
        "            for batch in train_dataloader:\n",
        "                \n",
        "                # add batch to GPU\n",
        "                batch = tuple(t.to(device) for t in batch) \n",
        "                \n",
        "                input_ids, attention_mask, loss_mask, start_labels, end_labels = batch\n",
        "                                \n",
        "                start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                \n",
        "                # calculate loss\n",
        "                loss = tweet_loss(start_logits, start_labels, end_logits, end_labels, loss_mask)\n",
        "                \n",
        "                # calculate gradients\n",
        "                loss.backward()               \n",
        "                # update model param\n",
        "                optimizer.step()\n",
        "                # update lr\n",
        "                scheduler.step()\n",
        "                # clean the gradients\n",
        "                model.zero_grad()\n",
        "            \n",
        "        # save validation and test logits\n",
        "        model.eval()\n",
        "        \n",
        "        val_fold_start_logits = []\n",
        "        val_fold_end_logits = []\n",
        "        \n",
        "        for batch in val_dataloader:\n",
        "            # add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch) \n",
        "                \n",
        "            input_ids, attention_mask, loss_mask, start_labels, end_labels = batch\n",
        "    \n",
        "            with torch.no_grad():\n",
        "                start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "            batch_start_logits = [start_logits[s].cpu() for s in range(input_ids.shape[0])]\n",
        "            batch_end_logits = [end_logits[s].cpu() for s in range(input_ids.shape[0])]\n",
        "    \n",
        "            val_fold_start_logits += batch_start_logits\n",
        "            val_fold_end_logits += batch_end_logits\n",
        "        \n",
        "        for e, idx in enumerate(val_idxs):\n",
        "            val_start_logits[idx] = val_fold_start_logits[e]\n",
        "            val_end_logits[idx] = val_fold_end_logits[e]\n",
        "        \n",
        "        test_fold_start_logits = []\n",
        "        test_fold_end_logits = []\n",
        "        \n",
        "        for batch in test_dataloader:\n",
        "            # add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch) \n",
        "                \n",
        "            input_ids, attention_mask, loss_mask = batch\n",
        "    \n",
        "            with torch.no_grad():\n",
        "                start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "            batch_start_logits = [start_logits[s].cpu() for s in range(input_ids.shape[0])]\n",
        "            batch_end_logits = [end_logits[s].cpu() for s in range(input_ids.shape[0])]\n",
        "    \n",
        "            test_fold_start_logits += batch_start_logits\n",
        "            test_fold_end_logits += batch_end_logits\n",
        "        \n",
        "        for e in range(len(test_dataset)):\n",
        "            test_start_logits[e] += test_fold_start_logits[e].squeeze(1)\n",
        "            test_end_logits[e] += test_fold_end_logits[e].squeeze(1)\n",
        "            \n",
        "        print(\"one fold over!\")\n",
        "           \n",
        "    return val_start_logits, val_end_logits, test_start_logits, test_end_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGIup-DBMOpn",
        "outputId": "57e12cac-f3b9-46d4-ed2d-dcc5582297ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one fold over!\n",
            "one fold over!\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# set the device to GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "val_start_logits, val_end_logits, test_start_logits, test_end_logits = train_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3rjPzPhMQYq"
      },
      "outputs": [],
      "source": [
        "jaccs = []\n",
        "predictions = []\n",
        "br = 0    \n",
        "for e, meta in enumerate(train_metadata):\n",
        "    \n",
        "    text = meta[\"text\"]\n",
        "    preprocessed_text = meta[\"preprocessed_text\"]\n",
        "    selected_text = meta[\"selected_text\"]\n",
        "    loss_mask = meta[\"loss_mask\"]\n",
        "    tokens = meta[\"tokens\"]\n",
        "    offsets = meta[\"offsets\"]\n",
        "    \n",
        "    actives = np.asarray(meta[\"loss_mask\"]).reshape(-1) == 1\n",
        "    start_probs = val_start_logits[e][actives].sigmoid()\n",
        "    end_probs = val_end_logits[e][actives].sigmoid()\n",
        "        \n",
        "    start_idx = start_probs.argmax().item()\n",
        "    end_idx = end_probs.argmax().item()\n",
        "    \n",
        "    if start_idx > end_idx:\n",
        "        br += 1\n",
        "        start_idx , end_idx = 0, start_probs.shape[0]-1\n",
        "    \n",
        "    start_max = start_probs.max().item()\n",
        "    end_max = end_probs.max().item()        \n",
        "\n",
        "    pp = preprocessed_text[offsets[start_idx][0]:offsets[end_idx][1]]\n",
        "    \n",
        "    if pp not in text or pp not in \" \" + text:\n",
        "        pp = prepare_predictions(pp)\n",
        "    \n",
        "    predictions.append(pp)\n",
        "    jaccs.append(jaccard(selected_text, pp))\n",
        "\n",
        "print('Mean Jaccard score for positive and negative sentiment tweets: ', round(statistics.mean(jaccs),4))\n",
        "print('Mean Jaccard score for neutral tweets: 0.9767')\n",
        "print('Mean Jaccard score: ', round(statistics.mean(jaccs)*0.5955+0.9767*0.4045,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLAZiWb_MS0Y"
      },
      "outputs": [],
      "source": [
        "test_raw[\"selected_text\"] = \"\" # make a column for predictions\n",
        "\n",
        "# make predictions for neutral sentiment\n",
        "test_raw.loc[test_raw.sentiment == \"neutral\", \"selected_text\"] = test_raw.loc[test_raw.sentiment == \"neutral\", \"text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqaqdorUMU4p"
      },
      "outputs": [],
      "source": [
        "# get positive and negative sentiment predictions\n",
        "\n",
        "predictions = []\n",
        "br = 0    \n",
        "for e, meta in enumerate(test_metadata):\n",
        "    \n",
        "    text = meta[\"text\"]\n",
        "    preprocessed_text = meta[\"preprocessed_text\"]\n",
        "    loss_mask = meta[\"loss_mask\"]\n",
        "    tokens = meta[\"tokens\"]\n",
        "    offsets = meta[\"offsets\"]\n",
        "    \n",
        "    actives = np.asarray(meta[\"loss_mask\"]).reshape(-1) == 1\n",
        "    start_probs = test_start_logits[e][actives].sigmoid()\n",
        "    end_probs = test_end_logits[e][actives].sigmoid()\n",
        "        \n",
        "    start_idx = start_probs.argmax().item()\n",
        "    end_idx = end_probs.argmax().item()\n",
        "    \n",
        "    if start_idx > end_idx:\n",
        "        br += 1\n",
        "        start_idx , end_idx = 0, start_probs.shape[0]-1       \n",
        "\n",
        "    pp = preprocessed_text[offsets[start_idx][0]:offsets[end_idx][1]]\n",
        "    \n",
        "    if pp not in text or pp not in \" \" + text:\n",
        "        pp = prepare_predictions(pp)\n",
        "    \n",
        "    predictions.append(pp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7TXe5cJMXAm"
      },
      "outputs": [],
      "source": [
        "# make predictions for neutral sentiment\n",
        "test_raw.loc[test_raw.sentiment != \"neutral\", \"selected_text\"] = predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR5oW_YqMYwf"
      },
      "outputs": [],
      "source": [
        "submission = test_raw.drop(columns = [\"text\", \"sentiment\"])\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWQA2Z1omfOw"
      },
      "outputs": [],
      "source": [
        "print(submission)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Tweet_Sentiment_Extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b302cdd1e032ee910f5c889c3360c28564c92ad4f326fc3102e39fbe47faee66"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
